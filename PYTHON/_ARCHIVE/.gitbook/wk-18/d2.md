# D2

{% embed url="https://gist.github.com/bgoonz/3deac2c8f0dfedb503eefc78a4dba394" caption="" %}

## Hash Tables

Hash tables \(also known as hash maps\) are associative arrays, or dictionaries, that allow for fast insertion, lookup and removal regardless of the number of items stored.

Here’s an example of how they can be used:

```text
HashTable phoneBook = new HashTable()
phoneBook.put("Adam", "(202) 555-0309")
phoneBook.put("Beth", "(335) 754-1215")

print("Adam’s number: " + phoneBook.get("Adam"))
```

Internally they are similar to card indexes: An item can be found quickly by first jumping to its approximate location, and then searching locally from there.

![](../.gitbook/assets/image.png)

### Representation

The simplest way to implement a hash table is to use an **array of linked lists**.

Each array cell is called a **bucket**, and each list node stores a **key-value pair**.

Following the analogy from the previous section, the array cells that can be accessed quickly can be thought of as index cards, and nodes in the list as data cards.

![](../.gitbook/assets/image%20%281%29.png)

### Finding the right bucket

Which bucket a given key belongs to, is determined by a **hash function** as follows…

_bucket index_ = hash\(_key_\) % length\(_bucket array_\)

…where % denotes the remainder operator.

For efficiency reasons, an additional variable is used to keep track of the current total number of key-value pairs.☞

There are other ways to implement hash tables, but this is the representation that will be used here to introduce the subject. See section [Other Collision Resolution Strategies](https://programming.guide/hash-tables.html#other-impl) for alternatives.

## Insertion

Here's how a mapping from key `k` to value `v` is inserted in a hash table:

**1.Rehash if load factor is too high. \(Explained in separate section.\)**

rehash_if_needed\(\)

**2.Compute the hash for the key**

h = hash\(k\)

**3.Use remainder operator to compute the bucket index**

i = h % length\(buckets\)

**4.Look for key among existing nodes. If found update value of existing node.**

\*\*\*\*

\*\*\*\*

\*\*\*\*

### Lookup

Here's how the value for a key `k` is retrieved from a hash table:1.Compute the hash for the keyh = hash\(k\)2.Use remainder operator to compute the bucket indexi = h % length\(buckets\)3.Search through bucket linearlyn = buckets\[i\]  
while n ≠ NULL:  
if n.key == k:  
return n.value  
n = n.next  
return NOT_FOUNDWrongkeyKeyfoundReturnvaluek,vk,vk,v

### Remove

Here's how a key-value pair is removed from a hash table:1.Compute the hash for the keyh = hash\(k\)2.Use remainder operator to compute the bucket indexi = h % length\(buckets\)3.Search through bucket linearly to find keylast = NULL  
n = buckets\[i\]  
while n ≠ NULL:  
if n.key == k:  
if last == NULL:  
buckets\[i\] = n.next  
else:  
last.next = n.next  
n = n.next

### Rehashing

As the lengths of the linked lists grow, the average lookup time increases.

To keep the linked lists short and lookups fast, the number of buckets must be increased \(and nodes redistributed\). This is called **rehashing**.1.Allocate a new bucket array.new_size = 2 \* length\(buckets\)  
new_buckets = new Bucket\[new_size\]2.Reinsert all elements in new array.for i in 0..buckets.length - 1:  
while buckets\[i\] ≠ NULL:  
n = buckets\[i\]  
buckets\[i\] = n.next  
h = hash\(n.k\)  
i = h % new_length  
n.next = new_buckets\[i\]  
new_buckets\[i\] = n  
3.Replace old array with new arraybuckets = new_bucketsbucketsnew_buckets

During rehashing the number of buckets is increased by some factor, typically 2. As shown in the article [Hash Tables: Complexity](https://programming.guide/hash-tables-complexity.html), it is important that the growth is exponential.

### Traversal

1.For each bucket…for i in 0..buckets.length - 1:2.…traverse each node n = buckets\[i\]:  
while n ≠ NULL:  
process\(n\)  
n = n.next

Note that the order in which the key-value pairs are traversed is **unpredictable** due to the nature of hashing and rehashing.

### Load Factor and Capacity

The **load factor** is the average number of key-value pairs per bucket.load factor=total number of key-value pairsnumber of buckets

It is when the load factor reaches a given limit that rehashing kicks in. Since rehashing increases the number of buckets, it reduces the load factor.

The load factor limit is usually configurable and offers a **tradeoff between time and space costs**.Lower limitMore empty bucketsMore memory overheadHigher limitLarger bucketsSlower lookups

The default load factor for a Java [`HashMap`](https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html) is 0.75 and for a C\# [`Hashtable`](https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8) it’s 1.0.

The **capacity** is the maximum number of key-value pairs for the given load factor limit and current bucket count.capacity=number of buckets×load factor limit

Since rehashing increases the number of buckets, it increases the capacity.

The default initial capacity for a Java [`HashMap`](https://docs.oracle.com/javase/8/docs/api/java/util/HashMap.html) is 12 and for a C\# [`Hashtable`](https://docs.microsoft.com/en-us/dotnet/api/system.collections.hashtable?view=netframework-4.8) it’s 0, i.e. the bucket array is initialized lazily upon first insertion.

**Example:** Here’s the structure of a hash table, configured with load factor limit of 4.Currentload factor:24 / 8 = 3Configuredlimit:4Current capacity:8 × 4 = 32

### Complexity Analysis

As is clear from the way insert, lookup and remove works, the run time is proportional to the length of the linked lists. In worst case all keys hash to the same bucket, i.e. the whole data structure becomes equivalent to a linked list. This means that all operations run in _O_\(_n_\).

Assuming however that keys are dispersed evenly among the buckets, it can be shown that the complexity of the expected run time is _O_\(1\) for all operations.

See article [Hash Tables: Complexity](https://programming.guide/hash-tables-complexity.html) for a detailed analysis.

### Other Collision Resolution Strategies <a id="other-impl"></a>

When two keys hash to the same bucket, i.e. whenhash\(\_k_1\) ≡ hash\(\_k_2\) \(mod number of buckets\)

it’s called a **collision**. The collision handling strategy described so far \(one linked list per bucket\) is an example of **closed addressing** using separate chains. Instead of linked lists, one can also use binary search trees, or as in the case of Java 9, a linked list up to a certain limit, and then convert it to a BST when more elements are added.

The alternative, **open addressing**, is to store all key-value pairs directly in the hash table array, i.e. there's at most one element per bucket. \(The size of the array must always be at least as large as the number of elements stored.\)

## Hash Tables: Complexity

This article is written with separate chaining and closed addressing in mind, specifically implementations based on **arrays of linked lists**. Most of the analysis however applies to other techniques, such as basic open addressing implementations.☞

Only operations that scale with the number of elements _n_ are considered in the analysis below. In particular, the hash function is assumed to run in constant time.

### Length of chains

As is clear from the way lookup, insert and remove works, the run time is proportional to the number of keys in the given chain. So, to analyze the complexity, we need to analyze the length of the chains.

#### Worst Case

If we're unlucky with the keys we encounter, or if we have a poorly implemented hash function, all keys may hash to the same bucket.

This means that the **worst-case complexity of a hash table is the same as that of a linked list**: _O_\(_n_\) for insert, lookup and remove.

This is however a pathological situation, and the theoretical worst-case is often uninteresting in practice. When discussing complexity for hash tables the focus is usually on **expected** run time.

#### Uniform Hashing

The expected length of any given linked list depends on how the hash function spreads out the keys among the buckets. For the purpose of this analysis, we will assume that we have an ideal hash function. This is a common assumption to make. So common in fact, that it has a name:

#### Simple Uniform Hashing Assumption

In a hash table with _m_ buckets, each key is hashed to any given bucket…

- …with the same probability, 1 / *m*
- …independently of which bucket any other key is hashed to.

With SUHA the keys are distributed **uniformly** and the expected length of any given linked list is therefore _n_ / _m_.

#### The Magic Part

As you may recall, the _n_ / _m_ ratio is called the load factor, and that rehashing guarantees that this is bound by the configured load factor limit. \(See [Hash Table Load Factor and Capacity](https://programming.guide/hash-table-load-factor-and-capacity.html).\) Since the load factor limit is constant, **the expected length of all chains can be considered constant**.☞

**A common misconception** is that SUHA implies constant time worst case complexity. SUHA however, does not say that all keys _will_ be distributed uniformly, only that the probability distribution is uniform. Even with a uniform probability, it is still _possible_ for all keys to end up in the same bucket, thus worst case complexity is still linear.

### Insert

An insertion will search through one bucket linearly to see if the key already exists. This runs in _O_\(_n_ / _m_\) which we know from the previous section is _O_\(1\). If the key is found, a value is updated, if not, a new node is appended to the list. Regardless of which, this part is in _O_\(1\).

If we're unlucky, rehashing is required before all that. Since rehashing performs _n_ constant time insertions, it runs in Θ\(_n_\).

That being said, rehashes are rare. In fact, they are so rare that _in average_ insertion still runs in constant time. We say that the **amortized time complexity for insert is** _**O**_**\(1\)**.

**Proof:** Suppose we set out to insert _n_ elements and that rehashing occurs at each power of two. Let's assume also that _n_ is a power of two so we hit the worst case scenario and have to rehash on the very last insertion.

We would have to rehash after inserting element 1, 2, 4, …, _n_. Since each rehashing reinserts all current elements, we would do, in total, 1 + 2 + 4 + 8 + … + _n_ = 2n − 1 extra insertions due to rehashing. In other words, all rehashing necessary incurs an average overhead of less than 2 extra insertions per element.

We conclude that despite the growing cost of rehashing, the average number of insertions per element stays constant.

### Lookup

A lookup will search through the chain of one bucket linearly. This is in _O_\(_n_ / _m_\) which, again, is _**O**_**\(1\)**.

### Removal

A removal will search through one bucket linearly. If the key is found, it is “unlinked” in constant time, so remove runs in _**O**_**\(1\)** as well.

If one wants to reclaim unused memory, removal may require allocating a smaller array and rehash into that. In this case removal runs in _O_\(_n_\) in worst case, and _O_\(1\) amortized.

### Traversal

There's no way to know which buckets are empty, and which ones are not, so all buckets must be traversed. This means traversal is Θ\(_n_ + _m_\).

In practice this is only relevant if the hash table is initialized with a very large capacity. After the first rehashing the number of buckets can be considered linearly proportional to the number of items, and traversal is _Θ_\(_n_\).

## inked Hash Table

A linked hash table is a **combination of a hash table and a linked list**. Nodes are arranged in buckets but also have a doubly linked list running through them.

One can use the hash table structure for fast hash based lookups, and the linked list for effirient traversal.

### Hash Table Traversal

There's no way to know which buckets are empty, and which ones are not, so all buckets must be traversed. This means traversal is Θ\(_n_ + _m_\).

In practice this is only relevant if the hash table is initialized with a very large capacity. After the first rehashing the number of buckets can be considered linearly proportional to the number of items, and traversal is _Θ_\(_n_\).

### Adding a linked list

If one wants to avoid the overhead due to empty buckets one can let a linked list run through all nodes. Whenever a node is added to the hash table, it's appended to this list.

**Example:** A linked hash table based on separate chaining.

This effectively trades some memory \(additional next / prev pointers\) for improved speed.

Since the overlay list structure is doubly linked, append and remove are constant time operations, so it does not affect the complexity of other operations.

The [`LinkedHashMap`](https://docs.oracle.com/javase/8/docs/api/java/util/LinkedHashMap.html) in the Java API is an example of this technique.
